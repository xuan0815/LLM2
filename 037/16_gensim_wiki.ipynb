{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Word2Vec實作**\n",
        "\n",
        "\n",
        "*   字詞所代表的意義非常多元，在不同狀況下，會代表不同意思。要把多元意思用單一向量表示，則必須要進行word embedding的動作，也就是把高維向量降為低維向量的過程\n",
        "*   之前介紹過，利用分散式表示法來表達字詞向量，例如PMI、SVD..統計法..等\n",
        "*   2013年神經網路盛行後，Tomas Mikolov利用神經網路訓練方式，來獲得字詞的表達向量，獲得很棒的成果。一般認為是利用神經網路模擬人類的理解能力，獲得不錯的分布空間所得到的成果。\n",
        "*   本範例以維基百科wiki部分資料作範例\n",
        "*   資料來源：https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2\n",
        "*   利用結巴分詞(jieba)進行斷詞，gensim套件進行word2vec計算\n",
        "*   本範例約需1小時長時間執行\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_XFGQnwy9gW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWq-QEzeyyIO",
        "outputId": "683c7ae8-cb2a-4f7e-e256-8f5164f4d27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-04 04:50:11--  https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.b\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-01-04 04:50:12 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dumps.wikimedia.org/zhwiki/20231201/zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##opencc是繁簡轉換工具"
      ],
      "metadata": {
        "id": "ZsFilzUmzwQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencc-python-reimplemented # 安裝套件"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V37r4YR1zyoM",
        "outputId": "bec8690a-b67d-48a8-ac99-df940872f364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/481.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/481.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m368.6/481.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gensim是訓練word2vec的函式庫"
      ],
      "metadata": {
        "id": "VCwFzMWS0A5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "wiki_corpus = WikiCorpus('zhwiki-20231201-pages-articles-multistream1.xml-p1p187712.bz2',"
      ],
      "metadata": {
        "id": "5VgkUKMI0HXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_corpus"
      ],
      "metadata": {
        "id": "bJT4LI460IUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(wiki_corpus.get_texts()))[:10]"
      ],
      "metadata": {
        "id": "prSb1OuJ0J93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##把wiki的資料檔案，轉換成連續文字的txt檔案"
      ],
      "metadata": {
        "id": "be6IaL930LjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_num = 0\n",
        "\n",
        "# 開啟 'wiki_text.txt' 檔案，使用 utf-8 編碼方式寫入\n",
        "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
        "    # 迴圈處理從 wiki_corpus 取得的每一篇文章\n",
        "    for text in wiki_corpus.get_texts():\n",
        "        # 將文章內容以空格連接，並寫入檔案，每行為一篇文章\n",
        "        f.write(' '.join(text) + '\\n')\n",
        "\n",
        "        # 文章數量加一\n",
        "        text_num += 1\n",
        "\n",
        "        # 每處理 10,000 篇文章印出一條訊息\n",
        "        if text_num % 10000 == 0:\n",
        "            print('{} articles processed.'.format(text_num))\n",
        "\n",
        "    # 最後印出總處理文章數\n",
        "    print('{} articles processed.'.format(text_num))"
      ],
      "metadata": {
        "id": "QuyCRQVU0Qnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "from opencc import OpenCC\n",
        "\n",
        "# 初始化 OpenCC，設定轉換為繁體中文\n",
        "cc = OpenCC('s2t')\n",
        "\n",
        "# 讀取 'wiki_text.txt' 檔案中的文本資料\n",
        "train_data = open('wiki_text.txt', 'r', encoding='utf-8').read()\n",
        "\n",
        "# 將文本進行簡繁體轉換\n",
        "train_data = cc.convert(train_data)\n",
        "\n",
        "# 使用 jieba 對文本進行中文斷詞\n",
        "train_data = jieba.lcut(train_data)\n",
        "\n",
        "# 移除空白字詞\n",
        "train_data = [word for word in train_data if word != '']\n",
        "\n",
        "# 將斷詞結果以空格連接成字串\n",
        "train_data = ' '.join(train_data)\n",
        "\n",
        "# 將處理後的文本保存到 'seg.txt' 檔案中\n",
        "open('seg.txt', 'w', encoding='utf-8').write(train_data)\n"
      ],
      "metadata": {
        "id": "pM8_8MQt0buV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# 設定\n",
        "seed = 666\n",
        "sg = 0  # Skip-gram 模型\n",
        "window_size = 10\n",
        "min_count = 1\n",
        "workers = 8\n",
        "batch_words = 10000\n",
        "\n",
        "# 從 'seg.txt' 讀取處理後的文本資料\n",
        "train_data = word2vec.LineSentence('seg.txt')\n",
        "\n",
        "# 使用 Word2Vec 訓練詞向量模型\n",
        "model = word2vec.Word2Vec(\n",
        "    train_data,\n",
        "    min_count=min_count,\n",
        "    workers=workers,\n",
        "    window=window_size,\n",
        "    sg=sg,\n",
        "    seed=seed,\n",
        "    batch_words=batch_words\n",
        ")\n",
        "\n",
        "# 儲存訓練好的 Word2Vec 模型\n",
        "model.save('word2vec.model')"
      ],
      "metadata": {
        "id": "-pjeh9Ji0lCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# 載入先前訓練好的 Word2Vec 模型\n",
        "model = word2vec.Word2Vec.load('word2vec.model')\n",
        "\n",
        "# 指定詞語\n",
        "string = '電腦'\n",
        "print(string)\n",
        "\n",
        "# 列印與指定詞語最相似的詞語及相似度\n",
        "for item in model.wv.most_similar(string):\n",
        "    print(item)\n"
      ],
      "metadata": {
        "id": "RAnEagjG0p6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}